{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "##Utils"
      ],
      "metadata": {
        "id": "8vFBt7SPUOmV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Installation and requirements"
      ],
      "metadata": {
        "id": "hUtxPzOrsad1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "-3F5tW6Lytx9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2ccd6677-e9e5-44be-ed3a-be910ffc58b3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pdf2image\n",
            "  Downloading pdf2image-1.16.3-py3-none-any.whl (11 kB)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from pdf2image) (9.4.0)\n",
            "Installing collected packages: pdf2image\n",
            "Successfully installed pdf2image-1.16.3\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.10/dist-packages (4.8.0.76)\n",
            "Requirement already satisfied: numpy>=1.21.2 in /usr/local/lib/python3.10/dist-packages (from opencv-python) (1.23.5)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (9.4.0)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  tesseract-ocr-eng tesseract-ocr-osd\n",
            "The following NEW packages will be installed:\n",
            "  tesseract-ocr tesseract-ocr-eng tesseract-ocr-osd\n",
            "0 upgraded, 3 newly installed, 0 to remove and 19 not upgraded.\n",
            "Need to get 4,816 kB of archives.\n",
            "After this operation, 15.6 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-eng all 1:4.00~git30-7274cfa-1.1 [1,591 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-osd all 1:4.00~git30-7274cfa-1.1 [2,990 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr amd64 4.1.1-2.1build1 [236 kB]\n",
            "Fetched 4,816 kB in 1s (5,255 kB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78, <> line 3.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package tesseract-ocr-eng.\n",
            "(Reading database ... 120874 files and directories currently installed.)\n",
            "Preparing to unpack .../tesseract-ocr-eng_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-eng (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr-osd.\n",
            "Preparing to unpack .../tesseract-ocr-osd_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-osd (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr.\n",
            "Preparing to unpack .../tesseract-ocr_4.1.1-2.1build1_amd64.deb ...\n",
            "Unpacking tesseract-ocr (4.1.1-2.1build1) ...\n",
            "Setting up tesseract-ocr-eng (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr-osd (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr (4.1.1-2.1build1) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Collecting pytesseract\n",
            "  Downloading pytesseract-0.3.10-py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.10/dist-packages (from pytesseract) (23.2)\n",
            "Requirement already satisfied: Pillow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from pytesseract) (9.4.0)\n",
            "Installing collected packages: pytesseract\n",
            "Successfully installed pytesseract-0.3.10\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following NEW packages will be installed:\n",
            "  poppler-utils\n",
            "0 upgraded, 1 newly installed, 0 to remove and 19 not upgraded.\n",
            "Need to get 186 kB of archives.\n",
            "After this operation, 696 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 poppler-utils amd64 22.02.0-2ubuntu0.2 [186 kB]\n",
            "Fetched 186 kB in 0s (446 kB/s)\n",
            "Selecting previously unselected package poppler-utils.\n",
            "(Reading database ... 120921 files and directories currently installed.)\n",
            "Preparing to unpack .../poppler-utils_22.02.0-2ubuntu0.2_amd64.deb ...\n",
            "Unpacking poppler-utils (22.02.0-2ubuntu0.2) ...\n",
            "Setting up poppler-utils (22.02.0-2ubuntu0.2) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Collecting sentence-transformers\n",
            "  Downloading sentence-transformers-2.2.2.tar.gz (85 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.0/86.0 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting transformers<5.0.0,>=4.6.0 (from sentence-transformers)\n",
            "  Downloading transformers-4.35.0-py3-none-any.whl (7.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.9/7.9 MB\u001b[0m \u001b[31m49.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.66.1)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (2.1.0+cu118)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (0.16.0+cu118)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.23.5)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.11.3)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (3.8.1)\n",
            "Collecting sentencepiece (from sentence-transformers)\n",
            "  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m78.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting huggingface-hub>=0.4.0 (from sentence-transformers)\n",
            "  Downloading huggingface_hub-0.19.0-py3-none-any.whl (311 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m311.2/311.2 kB\u001b[0m \u001b[31m38.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (3.13.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (2023.6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (2.31.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (6.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (4.5.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (23.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers) (3.1.2)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers) (2.1.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (2023.6.3)\n",
            "Collecting tokenizers<0.15,>=0.14 (from transformers<5.0.0,>=4.6.0->sentence-transformers)\n",
            "  Downloading tokenizers-0.14.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m103.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting safetensors>=0.3.1 (from transformers<5.0.0,>=4.6.0->sentence-transformers)\n",
            "  Downloading safetensors-0.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m86.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->sentence-transformers) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->sentence-transformers) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (3.2.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->sentence-transformers) (9.4.0)\n",
            "Collecting huggingface-hub>=0.4.0 (from sentence-transformers)\n",
            "  Downloading huggingface_hub-0.17.3-py3-none-any.whl (295 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m295.0/295.0 kB\u001b[0m \u001b[31m38.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.6.0->sentence-transformers) (2.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (2023.7.22)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.6.0->sentence-transformers) (1.3.0)\n",
            "Building wheels for collected packages: sentence-transformers\n",
            "  Building wheel for sentence-transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sentence-transformers: filename=sentence_transformers-2.2.2-py3-none-any.whl size=125923 sha256=34169ec633b99bd36616c425ceb5d7f156cafcfee733eb13504442c1dced399e\n",
            "  Stored in directory: /root/.cache/pip/wheels/62/f2/10/1e606fd5f02395388f74e7462910fe851042f97238cbbd902f\n",
            "Successfully built sentence-transformers\n",
            "Installing collected packages: sentencepiece, safetensors, huggingface-hub, tokenizers, transformers, sentence-transformers\n",
            "Successfully installed huggingface-hub-0.17.3 safetensors-0.4.0 sentence-transformers-2.2.2 sentencepiece-0.1.99 tokenizers-0.14.1 transformers-4.35.0\n",
            "Collecting rank_bm25\n",
            "  Downloading rank_bm25-0.2.2-py3-none-any.whl (8.6 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from rank_bm25) (1.23.5)\n",
            "Installing collected packages: rank_bm25\n",
            "Successfully installed rank_bm25-0.2.2\n"
          ]
        }
      ],
      "source": [
        "!pip install pdf2image\n",
        "!pip install opencv-python\n",
        "!pip install Pillow\n",
        "!sudo apt install tesseract-ocr\n",
        "!pip install pytesseract\n",
        "!apt-get install poppler-utils\n",
        "!pip install -U sentence-transformers\n",
        "!pip install rank_bm25"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Creating useful folders\n",
        "!mkdir img\n",
        "!mkdir texts\n",
        "!mkdir -p texts/page_splitted\n",
        "!mkdir -p texts/full_text"
      ],
      "metadata": {
        "id": "1nBq2OcJQko7"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer, util\n",
        "from pdf2image import convert_from_path\n",
        "import re\n",
        "import cv2\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pytesseract\n",
        "from pytesseract import Output\n",
        "from matplotlib import pyplot as plt\n",
        "from rank_bm25 import BM25L, BM25Plus\n",
        "from sentence_transformers import CrossEncoder"
      ],
      "metadata": {
        "id": "P_QJeB4q8yHt"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Functions definition"
      ],
      "metadata": {
        "id": "hVL5s7VYr-eH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def phrases_from_txt(file_path, full_text = True, split_value = '.\\n'):\n",
        "  if full_text == False:\n",
        "    with open(file_path, 'r') as f:\n",
        "      txt = f.read()\n",
        "    phrases = txt.split(\"\\n\\n\\n\")\n",
        "    phrases = phrases[:-1] #removing last empty element of the list\n",
        "  else:\n",
        "    with open(file_path, 'r') as f:\n",
        "      txt = f.read()\n",
        "      phrases = txt.split(split_value)\n",
        "\n",
        "  return phrases"
      ],
      "metadata": {
        "id": "YU9fKh_2JCHq"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def split_at_colon(file_path):\n",
        "\n",
        "  with open(file_path, 'r') as f:\n",
        "      txt = f.read()\n",
        "\n",
        "  phrases  = txt.split(\": \")\n",
        "\n",
        "  for i in range(len(phrases )-1):\n",
        "    elem = phrases [i].split(\"\\n\")[-1]\n",
        "    phrases [i] = phrases [i].replace(elem, '')\n",
        "    phrases [i+1] = elem + \" \" + phrases [i+1] + \":\"\n",
        "\n",
        "  return phrases"
      ],
      "metadata": {
        "id": "o0U-1W4YfYvY"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_unwanted_text(text):\n",
        "  new_text = str(text)\n",
        "  new_text = re.sub('(\\:(-)?\\)|\\:(-)?\\(|<3|\\:(-)?\\/|\\:-\\/|\\:(-)?\\||\\:(-)?[pP]|\\s\\:+(-)?([0-9])?\\s|\\^\\^|\\s\\:+(-)?(\\D)?\\s)', '', new_text)  # removing smile with :\n",
        "  new_text = new_text.replace('__', '')\n",
        "  new_text = new_text.replace('“', '')\n",
        "  new_text = new_text.replace('”', '')\n",
        "  new_text = new_text.replace('\"', '')\n",
        "  new_text = new_text.replace('/', '')\n",
        "  new_text = new_text.replace('\\\\','/')\n",
        "  new_text = new_text.replace('\\n\\n',' ')\n",
        "  return new_text\n"
      ],
      "metadata": {
        "id": "CBkDSr-ENe1D"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def combined_list(list1, list2, desired_length):\n",
        "    common_elements = list(set(list1) & set(list2))  # Get common elements between the two lists\n",
        "    list1 = [x for x in list1 if x not in common_elements]\n",
        "    list2 = [x for x in list2 if x not in common_elements]\n",
        "    combined_list = []\n",
        "    combined_list.extend(common_elements)\n",
        "    for i in range(min(len(list1), len(list2), desired_length)):\n",
        "        combined_list.append(list1[i])\n",
        "        combined_list.append(list2[i])\n",
        "    return combined_list[:desired_length]"
      ],
      "metadata": {
        "id": "hylNqP9yywoA"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def semantic_search(query, docs, model, metric, top_n = 10, doc_emb_available = False, doc_emb = 0):\n",
        "  #Load the model\n",
        "  model = SentenceTransformer(model)\n",
        "\n",
        "  #Encode query and documents\n",
        "  query_emb = model.encode(query)\n",
        "\n",
        "  if doc_emb_available == True:\n",
        "    doc_emb = doc_emb\n",
        "  else:\n",
        "    doc_emb = model.encode(docs)\n",
        "\n",
        "  #Compute the selected metric between query and all document embeddings\n",
        "  scores = metric(query_emb, doc_emb)[0].cpu().tolist()\n",
        "\n",
        "  #Combine docs & scores\n",
        "  doc_score_pairs = list(zip(docs, scores))\n",
        "\n",
        "  #doc_score_pairs = [(index + 1, doc, score) for index, (doc, score) in enumerate(doc_score_pairs)]\n",
        "\n",
        "  #Sort by decreasing score\n",
        "  doc_score_pairs = sorted(doc_score_pairs, key=lambda x: x[1], reverse=True)\n",
        "\n",
        "  #Select only the top_n documents\n",
        "  doc_score_pairs = doc_score_pairs[:top_n]\n",
        "\n",
        "  best_document_match = doc_score_pairs[0][0] #text of the best match (page of best match in case of page_splitting)\n",
        "\n",
        "  #Output passages & scores\n",
        "  for doc, score in doc_score_pairs:\n",
        "      print(score, '\\n', doc, '\\n\\n')\n",
        "  return doc_emb"
      ],
      "metadata": {
        "id": "5qB4zb8BUdJP"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def semantic_search_top_n(query, docs, model, metric, top_n = 3, doc_emb_available = False, doc_emb = 0):\n",
        "  #Load the model\n",
        "  model = SentenceTransformer(model)\n",
        "\n",
        "  #Encode query and documents\n",
        "  query_emb = model.encode(query)\n",
        "\n",
        "  if doc_emb_available == True:\n",
        "    doc_emb = doc_emb\n",
        "  else:\n",
        "    doc_emb = model.encode(docs)\n",
        "\n",
        "  #Compute the selected metric between query and all document embeddings\n",
        "  scores = metric(query_emb, doc_emb)[0].cpu().tolist()\n",
        "\n",
        "  #Combine docs & scores\n",
        "  doc_score_pairs = list(zip(docs, scores))\n",
        "\n",
        "  #doc_score_pairs = [(index + 1, doc, score) for index, (doc, score) in enumerate(doc_score_pairs)]\n",
        "\n",
        "  #Sort by decreasing score\n",
        "  doc_score_pairs = sorted(doc_score_pairs, key=lambda x: x[1], reverse=True)\n",
        "\n",
        "  #Choosing only the top_n documents\n",
        "  doc_score_pairs = doc_score_pairs[:top_n]\n",
        "\n",
        "  pages = [text[0] for text in doc_score_pairs]\n",
        "\n",
        "  concatenated_pages = ' '.join(pages)\n",
        "\n",
        "  print(concatenated_pages)\n",
        "  # Try opening the file in append mode\n",
        "  try:\n",
        "      with open(\"Text for LLM.txt\", \"a\") as file:\n",
        "          file.write('\\nDocument name: ' + pdf_name+ '\\n\\n' + concatenated_pages + \"\\n\\n\")\n",
        "  except FileNotFoundError:\n",
        "      # File doesn't exist, create it and append the text\n",
        "      with open(\"Text for LLM.txt\", \"w\") as file:\n",
        "          file.write(concatenated_pages + \"\\n\")\n",
        "\n",
        "  return doc_emb"
      ],
      "metadata": {
        "id": "ZoMmFP45Wiub"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def semantic_search_top_n_hybrid(query, docs, model, metric, top_n = 3, bm25model = BM25Plus, doc_emb_available = False, doc_emb = 0):\n",
        "\n",
        "  tokenized_corpus = [doc.split(\" \") for doc in docs]\n",
        "\n",
        "  bm25 = bm25model(tokenized_corpus)\n",
        "\n",
        "  tokenized_query = query.split(\" \")\n",
        "\n",
        "  scores_bm25 = bm25.get_scores(tokenized_query)\n",
        "\n",
        "  sorted_indices = np.argsort(scores_bm25)[::-1]\n",
        "\n",
        "  # Return the indices of the top n elements\n",
        "  top_n_indices_bm25 = sorted_indices[:top_n]\n",
        "  top_n_indices_bm25 = top_n_indices_bm25.tolist()\n",
        "\n",
        "  #Load the model\n",
        "  model = SentenceTransformer(model)\n",
        "\n",
        "  #Encode query and documents\n",
        "  query_emb = model.encode(query)\n",
        "\n",
        "  if doc_emb_available == True:\n",
        "    doc_emb = doc_emb\n",
        "  else:\n",
        "    doc_emb = model.encode(docs)\n",
        "\n",
        "  #Compute the selected metric between query and all document embeddings\n",
        "  scores_semantic = metric(query_emb, doc_emb)[0].cpu().tolist()\n",
        "\n",
        "  sorted_indices = np.argsort(scores_semantic)[::-1]\n",
        "\n",
        "  # Return the indices of the top n elements\n",
        "  top_n_indices_semantic = sorted_indices[:top_n]\n",
        "  top_n_indices_semantic = top_n_indices_semantic.tolist()\n",
        "\n",
        "  indices_top_n_documents = combined_list(top_n_indices_bm25, top_n_indices_semantic, top_n)\n",
        "\n",
        "  print(\"Top3 best results with semantic search: \", top_n_indices_semantic, '\\n')\n",
        "  print(\"Top3 best results with bm25 search: \", top_n_indices_bm25, '\\n')\n",
        "  print(\"Top3 best results with hybrid search: \", indices_top_n_documents, '\\n')\n",
        "\n",
        "  text = \"\"\n",
        "  for i in indices_top_n_documents:\n",
        "    text = text + phrases[i]\n",
        "\n",
        "  return doc_emb, text"
      ],
      "metadata": {
        "id": "xeMcOiAAy4bN"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def semantic_search_top_n_re_ranking(query, docs, model, metric, top_n = 3, num_docs = 20, model_cross = 'cross-encoder/ms-marco-MiniLM-L-6-v2', doc_emb_available = False, doc_emb = 0):\n",
        "  #Semantic search\n",
        "  #Load the model\n",
        "  model = SentenceTransformer(model)\n",
        "\n",
        "  #Encode query and documents\n",
        "  query_emb = model.encode(query)\n",
        "\n",
        "  if doc_emb_available == True:\n",
        "    doc_emb = doc_emb\n",
        "  else:\n",
        "    doc_emb = model.encode(docs)\n",
        "\n",
        "  #Compute the selected metric between query and all document embeddings\n",
        "  scores_semantic = metric(query_emb, doc_emb)[0].cpu().tolist()\n",
        "\n",
        "  sorted_indices = np.argsort(scores_semantic)[::-1]\n",
        "\n",
        "  # Return the indices of the top n elements\n",
        "  top_n_indices_semantic = sorted_indices[:num_docs]\n",
        "  top_n_indices_semantic = top_n_indices_semantic.tolist()\n",
        "\n",
        "  #Re-ranking\n",
        "  scores = []\n",
        "  model_cross = CrossEncoder(model_cross, max_length=512)\n",
        "  for i in top_n_indices_semantic:\n",
        "    scores.append((i,(model_cross.predict((query, docs[i])))))\n",
        "\n",
        "  sorted_scores = sorted(scores, key=lambda x: x[1], reverse=True)[:top_n]\n",
        "\n",
        "  print(\"Top results with semantic and re-ranking search: \", sorted_scores)\n",
        "  indices = [t[0] for t in sorted_scores]\n",
        "\n",
        "  text = \"\"\n",
        "  for i in indices:\n",
        "    text = text + phrases[i]\n",
        "\n",
        "  return doc_emb, text"
      ],
      "metadata": {
        "id": "-cS012si2Pfn"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Semantic search that outputs the best page according to the semantic search and both the previous and the following one concatenating all text in one\n",
        "\n",
        "def semantic_search_three_pages(query, docs, model, metric, doc_emb_available=False, doc_emb=0):\n",
        "    # Load the model\n",
        "    model = SentenceTransformer(model)\n",
        "\n",
        "    # Encode query and documents\n",
        "    query_emb = model.encode(query)\n",
        "\n",
        "    if doc_emb_available:\n",
        "        doc_emb = doc_emb\n",
        "    else:\n",
        "        doc_emb = model.encode(docs)\n",
        "\n",
        "    # Compute the selected metric between query and all document embeddings\n",
        "    scores = metric(query_emb, doc_emb)[0].cpu().tolist()\n",
        "\n",
        "    # Combine docs, scores, and indices\n",
        "    doc_score_index_triplets = list(zip(docs, scores, range(len(docs))))\n",
        "\n",
        "    # Find the index of the best match\n",
        "    best_match_index = max(doc_score_index_triplets, key=lambda x: x[1])[2]\n",
        "\n",
        "    # Find the index of the page before the best match\n",
        "    page_before_index = best_match_index - 1 if best_match_index > 0 else None\n",
        "\n",
        "    # Find the index of the page after the best match\n",
        "    page_after_index = best_match_index + 1 if best_match_index < len(docs) - 1 else None\n",
        "\n",
        "    # Sort by decreasing score\n",
        "    doc_score_index_triplets = sorted(doc_score_index_triplets, key=lambda x: x[1], reverse=True)\n",
        "\n",
        "    # Extract the text of the best match\n",
        "    best_document_match = doc_score_index_triplets[0][0]\n",
        "\n",
        "    # Concatenate the text of the page before the best match, best match, and the page after the best match\n",
        "    if page_before_index is not None:\n",
        "        best_document_match = docs[page_before_index] + best_document_match\n",
        "    if page_after_index is not None:\n",
        "        best_document_match += docs[page_after_index]\n",
        "\n",
        "    print(best_document_match)\n",
        "\n",
        "    return doc_emb"
      ],
      "metadata": {
        "id": "r3tgSTtdkllS"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Variables"
      ],
      "metadata": {
        "id": "ECBBxDB2_zLB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pdf_name = \"prova.pdf\"\n",
        "pdf_path = r\"/content/\" + pdf_name\n",
        "IMG_DIR = '/content/img/'\n",
        "full_text_file_path = '/content/texts/full_text/'+ \"full text \" + pdf_name.replace(\".pdf\", \".txt\")\n",
        "page_splitted_file_path = '/content/texts/page_splitted/'+ \"page splitted \" + pdf_name.replace(\".pdf\", \".txt\")"
      ],
      "metadata": {
        "id": "UP8LOH8A_08F"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Pytesseract"
      ],
      "metadata": {
        "id": "tTfiHEv4US-W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###First time converting pdf"
      ],
      "metadata": {
        "id": "nKjlaI9dAQSs"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "nemhiumhyxVg"
      },
      "outputs": [],
      "source": [
        "pages = convert_from_path(pdf_path, 400)\n",
        "\n",
        "i = 1\n",
        "for page in pages:\n",
        "    image_name = IMG_DIR + \"Page_\" + str(i) + \".jpg\"\n",
        "    page.save(image_name, \"JPEG\")\n",
        "    i = i+1"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stringTotal=[]\n",
        "custom_config = r'--oem 3 --psm 6'\n",
        "i=1\n",
        "for page in pages:\n",
        "  image = cv2.imread(IMG_DIR + 'Page_' + str(i) + '.jpg')\n",
        "  writePdf=(pytesseract.image_to_string(image, config=custom_config))\n",
        "  page_name = 'Page number: ' + str(i)\n",
        "  #writePdf = '\\n' + page_name + '\\n' + writePdf #useful only for page splitting\n",
        "  stringTotal.append(writePdf)\n",
        "  i=i+1\n",
        "\n",
        "pdfTotal=' '.join(stringTotal)\n"
      ],
      "metadata": {
        "id": "VHc8UNcf2A_c"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Sanity check\n",
        "len(pages)"
      ],
      "metadata": {
        "id": "SkebbHZZdQc2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6bf7a62a-9ba0-41c1-9e86-9acb4511f1af"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "13"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open(full_text_file_path, 'w') as f:\n",
        "  f.write(pdfTotal)"
      ],
      "metadata": {
        "id": "MrDLSjv1buSE"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Splitting at each page\n",
        "phrases = stringTotal\n",
        "\n",
        "with open(page_splitted_file_path, 'w') as f:\n",
        "  for phrase in phrases:\n",
        "        f.write(f\"{phrase}\\n\\n\\n\")\n"
      ],
      "metadata": {
        "id": "ldtNdCXY7UgR"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Starting from an already converted pdf"
      ],
      "metadata": {
        "id": "IKiR6ItOAZ-r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#split_at_colon(full_text_file_path)"
      ],
      "metadata": {
        "id": "NLWpMa_Vdgld"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "phrases = phrases_from_txt(page_splitted_file_path,full_text = False) #For page splitting\n",
        "#phrases = phrases_from_txt(full_text_file_path, split_value = \".\\n\") #For line splitting at specific character (split_value)"
      ],
      "metadata": {
        "id": "QHUExDv9RVfF"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Sanity check\n",
        "len(phrases)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4TQ7go9DRo2i",
        "outputId": "4973c54a-10a4-443b-902e-71a2e4802db9"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "13"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Text processing"
      ],
      "metadata": {
        "id": "Xzbc4O41UZ6u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "phrases=[remove_unwanted_text(text) for text in phrases]"
      ],
      "metadata": {
        "id": "by8xWHB6Y2S4"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Semantic search\n"
      ],
      "metadata": {
        "id": "N4M-z7QvpcxD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "doc_emb, text = semantic_search_top_n_re_ranking(\"Death benefit increase\", phrases, 'sentence-transformers/all-MiniLM-L6-v2',util.cos_sim, top_n=2, num_docs = 20)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rSxPzhmF3oyr",
        "outputId": "d9b03c47-dc0b-453b-876c-571fa65382c4"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top results with semantic and re-ranking search:  [(5, 3.7563944), (6, 0.23575786)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        },
        "id": "gcaiYY2j27af",
        "outputId": "03e6181d-5e9c-4030-d697-b16635b7e89a"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'C Benefits provided by this Policy\\nC1 Death Benefit\\nUpon the death of the Designated Life Insured, the Death Benefit specified in the Policy Schedule is payable to the\\nBeneficiary subject to the terms and provisions of this Policy.\\nIf your Coverage Option is Joint First-to-die, and a second Life Insured dies within sixty (60) days of the Designated\\nLife Insured, we will pay a supplementary Death Benefit to the Beneficiary provided that the surviving Life Insured\\nhas not exercised the Survivor Privilege described in provision E 1. The supplementary Death Benefit payable will\\nbe equal to the Death Benefit under this Policy the day the Designated Life Insured died, excluding any Riders. We\\nwill not pay a Death Benefit for any subsequent deaths.\\nIf two or more Lives Insured die at the same time, or under circumstances that make it uncertain who died first, we\\nwill deem a younger insured to have survived an older insured and the oldest such Life Insured will be deemed the\\nDesignated Life Insured.\\nC2 Changing your Death Benefit\\nC 2.1 Increasing your Death Benefit\\nYou may apply for an increase to your Death Benefit at any time, subject/to our administrative rules and the\\nfollowing conditions:\\n1. The minimum increase allowed is $10,000, and you must submit evidence of insurability, satisfactory to us, for\\neach increase.\\n2. Each increase will consist of an additional Coveragé)with its /own Coverage Date. The premium rates\\napplicable to each Life Insured under the new additional Coverage will be based on the following:\\na) The amount of additional Coverage;\\nb) The premium rates we then offer for this insurance plan;\\nc) The Attained Age of the Life Insured at the timejof the change; and\\nd) The applicable Class and RiSk Classification used in calculating the Premiums for this additional\\nCoverage, based on our determination*of the insurability of each applicant at the time you applied for the\\nincrease.\\n3. The new Coverage will be.effective on the Monthly Processing Day on or following the date we have approved\\nyour application foram increase, and we will adjust your Premium effective on that day.\\n4. The period within which.we.may contest the validity of this Policy, and the exclusion from Coverage for suicide,\\nwill apply to the additional Coverag@from its Coverage Date, as described in provisions C 5 and F 6.\\nC 2.2 Reducing yourDeath Benefit\\nYou may request that we reduce your Death Benefit at any time, subject to our administrative rules and the following\\nconditions:\\n1. All Premiums are paid to the effective date of the reduction.\\n2. The reduced Death Benefit will be effective on the Monthly Processing Day following the date we have received\\nyour written request. We will adjust your Premium effective on that day.\\n3. The minimum decrease allowed is $10,000, and the remaining Death Benefit must not be less than the\\nminimum specified in our administrative rules.\\n4. lf you reduce your Death Benefit to less than $250,000, the applicable Class will be the Standard Class that\\ncorresponds to the smoking habit of the Life or Lives Insured.\\n5. If we have previously approved any additional Coverage for this policy as described in provision C 2.1, we will\\nreduce your Death Benefit sequentially starting with the most recently added Coverage.\\nTERM100ENG 10.2009 Page 9\\n\\x0cC Benefits provided by this Policy C3 When we will pay the Death Benefit\\nWe will pay the Death Benefit to the Beneficiary upon the death of the Designated Life Insured, subject to the terms\\nand conditions of this Policy, only after we receive any information we might reasonably request to evaluate the\\nclaim and evidence we consider sufficient to establish:\\na) That the Designated Life Insured died while this Policy was in force;\\nb) The cause and circumstances of the death;\\nc) The age of the Designated Life Insured;\\nd) The smoking habit of the Designated Life Insured on the Coverage Date; and\\ne) The right of the claimant to be paid.\\nWe will apply similar terms and conditions to Death Benefit payable under any Rider attached to this Policy. C4 When we will adjust the Death Benefit\\nIf a Premium is due at the time of death, we will deduct it from the Death Benefit.\\nIf the date of birth or sex of any Life Insured has been stated incorrectly, weyreserve the right to increase or\\ndecrease any Death Benefit payable to the amount that would have been provided by the Premium paid using the\\ncorrect age or sex. We will calculate the correct amount payable.at any time after the misstatement of age or sex is\\ndiscovered. C5 When we will not pay the Death Benefit\\nWe will not pay the Death Benefit if any Life Insured Commits suicide, while sane or insane, within two (2) years from\\nthe Coverage Date or any reinstatement date. We will refund the Premiums paid for this Policy, from the later of the\\nCoverage Date or the date of the last reinstatement, without interest, to you.\\nWe will not pay the Death Benefit if this Policy is declared void due to a material omission, misrepresentation or\\nfraud, as described in provision F 6. TERM100ENG 10.2009 Page 10\\n\\x0c'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LLM"
      ],
      "metadata": {
        "id": "aQONr3yp3ouC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install \"accelerate>=0.16.0,<1\" \"transformers[torch]>=4.28.1,<5\" \"torch>=1.13.1,<2\""
      ],
      "metadata": {
        "id": "orfL2Hcg3ntH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import pipeline\n",
        "\n",
        "generate_text = pipeline(model=\"databricks/dolly-v2-3b\", torch_dtype=torch.bfloat16, trust_remote_code=True, device_map=\"auto\")"
      ],
      "metadata": {
        "id": "mWjLTV9k3tNb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"Given the following text, tell me which is the MINIMUM increase allowed in death benefit: \" + text\n",
        "res = generate_text(prompt)\n",
        "print(res[0][\"generated_text\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ggVin_qy86u5",
        "outputId": "e876394f-c32b-4bae-c3a2-ed2060a98d74"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The minimum increase allowed is $10,000.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test stage"
      ],
      "metadata": {
        "id": "G3wOXtTGPIor"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tables\n"
      ],
      "metadata": {
        "id": "vq6YiPPu5GOc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tabula-py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ph7IWCqr5MEq",
        "outputId": "018745ad-cac2-4b43-e6a4-7e9502fd831a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting tabula-py\n",
            "  Downloading tabula_py-2.7.0-py3-none-any.whl (12.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.0/12.0 MB\u001b[0m \u001b[31m79.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas>=0.25.3 in /usr/local/lib/python3.10/dist-packages (from tabula-py) (1.5.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from tabula-py) (1.22.4)\n",
            "Collecting distro (from tabula-py)\n",
            "  Downloading distro-1.8.0-py3-none-any.whl (20 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.25.3->tabula-py) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.25.3->tabula-py) (2022.7.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas>=0.25.3->tabula-py) (1.16.0)\n",
            "Installing collected packages: distro, tabula-py\n",
            "Successfully installed distro-1.8.0 tabula-py-2.7.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tabula\n",
        "#tabula.convert_into(\"SLIP RC GRUPO SUMCAB 2022.pdf\",\"prova.csv\",pages = \"all\")\n",
        "df = tabula.read_pdf(\"/content/prova\", pages=\"all\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6XVYo4B85Lax",
        "outputId": "e1a711f5-df55-480f-91ed-e63dc0d483de"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tabula.io:Got stderr: Jun 14, 2023 1:42:00 PM org.apache.pdfbox.pdmodel.font.PDTrueTypeFont <init>\n",
            "WARNING: Using fallback font 'LiberationSans' for 'TimesNewRomanPSMT'\n",
            "Jun 14, 2023 1:42:02 PM org.apache.pdfbox.pdmodel.font.PDTrueTypeFont <init>\n",
            "WARNING: Using fallback font 'LiberationSans' for 'TimesNewRomanPSMT'\n",
            "Jun 14, 2023 1:42:04 PM org.apache.pdfbox.pdmodel.font.PDTrueTypeFont <init>\n",
            "WARNING: Using fallback font 'LiberationSans' for 'TimesNewRomanPSMT'\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df[1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 707
        },
        "id": "7bHPy1kBFSTr",
        "outputId": "518dc154-4fee-48b4-bc32-bd82af9a29f9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                     EXPLOTACIÓN Y PATRONAL (PARTE A)  \\\n",
              "0                DAÑOS POR INCENDIO, EXPLOSIÓN, AGUAS   \n",
              "1                                            LOCATIVA   \n",
              "2                    PATRONAL (SUBLÍMITE POR VÍCTIMA)   \n",
              "3                    R.C. DAÑOS A BIENES DE EMPLEADOS   \n",
              "4                                        R.C. CRUZADA   \n",
              "5                                    R.C. SUBSIDIARIA   \n",
              "6                       R.C. CONTAMINACIÓN ACCIDENTAL   \n",
              "7                          R.C. TECNICOS EN PLANTILLA   \n",
              "8                       R.C. SUBSIDIARIA DE VEHICULOS   \n",
              "9                               R.C. CARGA Y DESCARGA   \n",
              "10                      R.C. TRANSPORTE DE MERCANCIAS   \n",
              "11                                PRODUCTOS (PARTE B)   \n",
              "12                                     R.C. PRODUCTOS   \n",
              "13                                R.C. UNION Y MEZCLA   \n",
              "14  GASTOS DE MONTAJE/DESMONTAJE/SUSTITUCION (Incl...   \n",
              "15                       R.C. PATRIMONIALES PRIMARIOS   \n",
              "16                          OTRAS CLAUSULAS INCLUIDAS   \n",
              "17                       CLAUSULA DE RECOMPRA DE S.A.   \n",
              "18          CLAUSULA DE PROGRAMA INTERNACIONAL L.P.S.   \n",
              "19                               LIBERACION DE GASTOS   \n",
              "20                             R.C. DEFENSA Y FIANZAS   \n",
              "\n",
              "                               Unnamed: 0  \n",
              "0                                INCLUIDO  \n",
              "1                                INCLUIDO  \n",
              "2                               600.000 €  \n",
              "3   30.000 € STRO/AÑO Y 3.000 POR\\rOBJETO  \n",
              "4                                INCLUIDO  \n",
              "5                                INCLUIDO  \n",
              "6                                INCLUIDO  \n",
              "7                                INCLUIDO  \n",
              "8                                INCLUIDO  \n",
              "9                                INCLUIDO  \n",
              "10                               INCLUIDO  \n",
              "11                                    NaN  \n",
              "12                               INCLUIDO  \n",
              "13                               INCLUIDO  \n",
              "14                              400.000 €  \n",
              "15                              150.000 €  \n",
              "16                                    NaN  \n",
              "17                               INCLUIDO  \n",
              "18                               INCLUIDO  \n",
              "19                               INCLUIDO  \n",
              "20                               INCLUIDO  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-1da02f62-0a3b-435a-b355-4c9ce9c6d9f8\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>EXPLOTACIÓN Y PATRONAL (PARTE A)</th>\n",
              "      <th>Unnamed: 0</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>DAÑOS POR INCENDIO, EXPLOSIÓN, AGUAS</td>\n",
              "      <td>INCLUIDO</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>LOCATIVA</td>\n",
              "      <td>INCLUIDO</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>PATRONAL (SUBLÍMITE POR VÍCTIMA)</td>\n",
              "      <td>600.000 €</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>R.C. DAÑOS A BIENES DE EMPLEADOS</td>\n",
              "      <td>30.000 € STRO/AÑO Y 3.000 POR\\rOBJETO</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>R.C. CRUZADA</td>\n",
              "      <td>INCLUIDO</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>R.C. SUBSIDIARIA</td>\n",
              "      <td>INCLUIDO</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>R.C. CONTAMINACIÓN ACCIDENTAL</td>\n",
              "      <td>INCLUIDO</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>R.C. TECNICOS EN PLANTILLA</td>\n",
              "      <td>INCLUIDO</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>R.C. SUBSIDIARIA DE VEHICULOS</td>\n",
              "      <td>INCLUIDO</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>R.C. CARGA Y DESCARGA</td>\n",
              "      <td>INCLUIDO</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>R.C. TRANSPORTE DE MERCANCIAS</td>\n",
              "      <td>INCLUIDO</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>PRODUCTOS (PARTE B)</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>R.C. PRODUCTOS</td>\n",
              "      <td>INCLUIDO</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>R.C. UNION Y MEZCLA</td>\n",
              "      <td>INCLUIDO</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>GASTOS DE MONTAJE/DESMONTAJE/SUSTITUCION (Incl...</td>\n",
              "      <td>400.000 €</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>R.C. PATRIMONIALES PRIMARIOS</td>\n",
              "      <td>150.000 €</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>OTRAS CLAUSULAS INCLUIDAS</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>CLAUSULA DE RECOMPRA DE S.A.</td>\n",
              "      <td>INCLUIDO</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>CLAUSULA DE PROGRAMA INTERNACIONAL L.P.S.</td>\n",
              "      <td>INCLUIDO</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>LIBERACION DE GASTOS</td>\n",
              "      <td>INCLUIDO</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>R.C. DEFENSA Y FIANZAS</td>\n",
              "      <td>INCLUIDO</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-1da02f62-0a3b-435a-b355-4c9ce9c6d9f8')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-1da02f62-0a3b-435a-b355-4c9ce9c6d9f8 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-1da02f62-0a3b-435a-b355-4c9ce9c6d9f8');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 102
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "diction = df[1].to_dict()\n",
        "dic_json = json.dumps(diction)\n",
        "dic_json"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 209
        },
        "id": "qo5fAaUrRwSX",
        "outputId": "a27343b7-48c9-4204-86a4-325e0de23e2c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'{\"EXPLOTACI\\\\u00d3N Y PATRONAL (PARTE A)\": {\"0\": \"DA\\\\u00d1OS POR INCENDIO, EXPLOSI\\\\u00d3N, AGUAS\", \"1\": \"LOCATIVA\", \"2\": \"PATRONAL (SUBL\\\\u00cdMITE POR V\\\\u00cdCTIMA)\", \"3\": \"R.C. DA\\\\u00d1OS A BIENES DE EMPLEADOS\", \"4\": \"R.C. CRUZADA\", \"5\": \"R.C. SUBSIDIARIA\", \"6\": \"R.C. CONTAMINACI\\\\u00d3N ACCIDENTAL\", \"7\": \"R.C. TECNICOS EN PLANTILLA\", \"8\": \"R.C. SUBSIDIARIA DE VEHICULOS\", \"9\": \"R.C. CARGA Y DESCARGA\", \"10\": \"R.C. TRANSPORTE DE MERCANCIAS\", \"11\": \"PRODUCTOS (PARTE B)\", \"12\": \"R.C. PRODUCTOS\", \"13\": \"R.C. UNION Y MEZCLA\", \"14\": \"GASTOS DE MONTAJE/DESMONTAJE/SUSTITUCION (Incluido\\\\rproducto de Terceros)\", \"15\": \"R.C. PATRIMONIALES PRIMARIOS\", \"16\": \"OTRAS CLAUSULAS INCLUIDAS\", \"17\": \"CLAUSULA DE RECOMPRA DE S.A.\", \"18\": \"CLAUSULA DE PROGRAMA INTERNACIONAL L.P.S.\", \"19\": \"LIBERACION DE GASTOS\", \"20\": \"R.C. DEFENSA Y FIANZAS\"}, \"Unnamed: 0\": {\"0\": \"INCLUIDO\", \"1\": \"INCLUIDO\", \"2\": \"600.000 \\\\u20ac\", \"3\": \"30.000 \\\\u20ac STRO/A\\\\u00d1O Y 3.000 POR\\\\rOBJETO\", \"4\": \"INCLUIDO\", \"5\": \"INCLUIDO\", \"6\": \"INCLUIDO\", \"7\": \"INCLUIDO\", \"8\": \"INCLUIDO\", \"9\": \"INCLUIDO\", \"10\": \"INCLUIDO\", \"11\": NaN, \"12\": \"INCLUIDO\", \"13\": \"INCLUIDO\", \"14\": \"400.000 \\\\u20ac\", \"15\": \"150.000 \\\\u20ac\", \"16\": NaN, \"17\": \"INCLUIDO\", \"18\": \"INCLUIDO\", \"19\": \"INCLUIDO\", \"20\": \"INCLUIDO\"}}'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 104
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "txt = \"\"\n",
        "for elem in range(len(df)):\n",
        "  txt = txt + (str(df[elem]).replace('\\n', '').replace('\\t', ''))\n",
        "\n",
        "txt = re.sub(r'\\s+', ' ', txt)"
      ],
      "metadata": {
        "id": "TptU9BJX68gO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(txt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "izk9dinoHCyP",
        "outputId": "9ac0bba6-c8d8-47bb-c4c9-f700a6c2cd36"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " COBERTURA LIMITE0 Límite por siniestro/año 3.000.000 € EXPLOTACIÓN Y PATRONAL (PARTE A) \\0 DAÑOS POR INCENDIO, EXPLOSIÓN, AGUAS 1 LOCATIVA 2 PATRONAL (SUBLÍMITE POR VÍCTIMA) 3 R.C. DAÑOS A BIENES DE EMPLEADOS 4 R.C. CRUZADA 5 R.C. SUBSIDIARIA 6 R.C. CONTAMINACIÓN ACCIDENTAL 7 R.C. TECNICOS EN PLANTILLA 8 R.C. SUBSIDIARIA DE VEHICULOS 9 R.C. CARGA Y DESCARGA 10 R.C. TRANSPORTE DE MERCANCIAS 11 PRODUCTOS (PARTE B) 12 R.C. PRODUCTOS 13 R.C. UNION Y MEZCLA 14 GASTOS DE MONTAJE/DESMONTAJE/SUSTITUCION (Incl... 15 R.C. PATRIMONIALES PRIMARIOS 16 OTRAS CLAUSULAS INCLUIDAS 17 CLAUSULA DE RECOMPRA DE S.A. 18 CLAUSULA DE PROGRAMA INTERNACIONAL L.P.S. 19 LIBERACION DE GASTOS 20 R.C. DEFENSA Y FIANZAS Unnamed: 0 0 INCLUIDO 1 INCLUIDO 2 600.000 € 3 30.000 € STRO/AÑO Y 3.000 POR\\rOBJETO 4 INCLUIDO 5 INCLUIDO 6 INCLUIDO 7 INCLUIDO 8 INCLUIDO 9 INCLUIDO 10 INCLUIDO 11 NaN 12 INCLUIDO 13 INCLUIDO 14 400.000 € 15 150.000 € 16 NaN 17 INCLUIDO 18 INCLUIDO 19 INCLUIDO 20 INCLUIDO Empty DataFrameColumns: [Riesgo de Explotación: Franquicia Fija de 600 € Productos: 1.500 € Retirada y Montaje/Desmontaje: 3.000 €]Index: []\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for elem in range(len(df)):\n",
        "  df[elem].to_csv('output.txt', sep='\\t', index=False, mode = 'a')"
      ],
      "metadata": {
        "id": "A5z4IilHA8I3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('output.txt') as file:\n",
        "  txt = file.read()"
      ],
      "metadata": {
        "id": "aPMCS3-aDQix"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = txt.replace('\\n', ' ').replace('\\t', ' ')\n",
        "text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "id": "uHL-HD4sDblP",
        "outputId": "270bccd7-e15a-48f4-8de0-6866fa235d0a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Country TSI PD TSI BI Premium PD Premium BI UK 969,970 250,985 0,673 0,327 DE 731,555 141,304 0,340 0,147 PL 519,970 84,895 0,256 0,071 IT 483,032 73,430 0,239 0,070 ES 492,525 46,307 0,236 0,037 NL 452,292 66,468 0,217 0,053 Various Countries 152,436 13,189 0,073 0,016 Total 3.801,780 676,578 2,033 0,722 Country TSI PD TSI BI Premium PD Premium BI DE 825,526 200,196 0,369 0,216 UK 316,164 59,183 0,196 0,118 Various Countries 111,809 29,269 0,074 0,047 Total 1.253,499 288,648 0,639 0,381 '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}